{
  "hash": "a96585ce9b42f168bb9a5d2a9b1c0d53",
  "result": {
    "markdown": "---\ntitle: \"La méthode des Kmeans\"\ndescription: |\n En analyse de données, il est parfois très utile de constituer des groupes homogènes d'observations c'est a dire d'individus partageant des similarités. La méthode des Kmeans permet de faire cela. Le présent billet de blog va introduire la méthode en utilisant R. \ntitle-block-banner: true\nimage: kmeans-files/kmeans-cover.jpg\ndate: \"2022-10-12\"\ncategories: [Kmeans,Clustering, ML]\nauthor:\n  - name: Ousmane Ouedraogo\n    url: https://oousmane.github.io\n    orcid: 0000-0002-6349-41468\neditor: visual\ncitation:\n url: https://oousmane.github.io/kmeans\nlicense: CC BY-SA\n---\n\n\n# Introduction\n\nLe partitionnement (clustering) est une méthode d'apprentissage non supervisée qui vise à rassembler les observations similaires d'un jeu de données en des sous-groupes bien distincts les uns des autres. \nCela est particulièrement utile pour le ciblage de clients, la définition de nouveau kit ou pack produit. \nUne méthode de clustering des plus utilisées est le kmeans (K-moyennes) qui vise à créer k groupes à partir de votre jeu de données. \nL'algorithme est assez simple et intuitif, je vous laisse une vidéo ici.\n\nDans ce tutoriel, nous allons apprendre comment implémenter la méthode des kmeans sur un jeu données notamment palmerpenguins.\n\n# Librairies utiles\n\nL'algorithme des kmeans, est disponible dans le R de base via la fonction `kmeans()` .\nNous aurons besoin du package `tidyverse` pour la manipulation des données et de `palmerpenguins` pour le jeu de données.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|warning: false\n#|message: false\nlibrary(factoextra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages\n────────────────────────────────── tidyverse\n1.3.2.9000 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ tibble    3.1.8      ✔ dplyr     1.0.10\n✔ tidyr     1.2.1      ✔ stringr   1.4.1 \n✔ readr     2.1.3      ✔ forcats   0.5.2 \n✔ purrr     0.3.5      ✔ lubridate 1.8.0 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidylog)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'tidylog'\n\nThe following objects are masked from 'package:dplyr':\n\n    add_count, add_tally, anti_join, count, distinct, distinct_all,\n    distinct_at, distinct_if, filter, filter_all, filter_at, filter_if,\n    full_join, group_by, group_by_all, group_by_at, group_by_if,\n    inner_join, left_join, mutate, mutate_all, mutate_at, mutate_if,\n    relocate, rename, rename_all, rename_at, rename_if, rename_with,\n    right_join, sample_frac, sample_n, select, select_all, select_at,\n    select_if, semi_join, slice, slice_head, slice_max, slice_min,\n    slice_sample, slice_tail, summarise, summarise_all, summarise_at,\n    summarise_if, summarize, summarize_all, summarize_at, summarize_if,\n    tally, top_frac, top_n, transmute, transmute_all, transmute_at,\n    transmute_if, ungroup\n\nThe following objects are masked from 'package:tidyr':\n\n    drop_na, fill, gather, pivot_longer, pivot_wider, replace_na,\n    spread, uncount\n\nThe following object is masked from 'package:stats':\n\n    filter\n```\n:::\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n```\n:::\n\n\n# Préparation des données\n\nLa méthode des kmeans requiert uniquement des données numériques et ne tolère pas de données manquantes.\nEn effet la similarité entre les observations est en fait une mesure de distance (par exemple euclidienne) entre elles.\nPlus la distance entre deux observations est petite, plus ces dernières sont similaires.\nAlors jetons regardons de près notre jeu de données\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n:::\n:::\n\n\nLe jeu de données `penguin`s est constitué de 344 observations et 8 variables dont trois catégorielles et 5 numériques.\nLa variable année bien que numérique n'est pas pertinente pour l'analyse.\nNous les enlèverons pour la suite, pour cela nous utiliserons la fonction `select_if()` de `dplyr` .\nLes données manquantes seront supprimés dans notre cas avec la fonction `drop_na()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: 'selection-numérique'\n#|message: true\npenguins_num <- penguins %>% \n  select(-year) %>% \n  select_if(is.numeric) %>% \n  drop_na()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nselect: dropped one variable (year)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nselect_if: dropped 3 variables (species, island, sex)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndrop_na: removed 2 rows (1%), 342 rows remaining\n```\n:::\n\n```{.r .cell-code}\nglimpse(penguins_num)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 342\nColumns: 4\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0…\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2…\n$ flipper_length_mm <int> 181, 186, 195, 193, 190, 181, 195, 193, 190, 186, 18…\n$ body_mass_g       <int> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250…\n```\n:::\n:::\n\n\nVoilà qui est fait, notre je de données est réduit à présent à quatres variables et 342 observations.\n\nLorsque les données sont exprimées dans des unités différentes comme ici grammes et millimètres, les variables ne sont pas intercomparables.\nPour résoudre cela, les données sont parfois centrées et réduites.\nNous pouvons faire cela avec la fonction `scale().`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_num <- penguins_num %>%\n  scale() %>% \n  as_tibble()\nglimpse(penguins_num)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 342\nColumns: 4\n$ bill_length_mm    <dbl> -0.8832047, -0.8099390, -0.6634077, -1.3227986, -0.8…\n$ bill_depth_mm     <dbl> 0.78430007, 0.12600328, 0.42983257, 1.08812936, 1.74…\n$ flipper_length_mm <dbl> -1.4162715, -1.0606961, -0.4206603, -0.5628905, -0.7…\n$ body_mass_g       <dbl> -0.563316704, -0.500969030, -1.186793445, -0.9374027…\n```\n:::\n:::\n\n\nLes données sont désormais sans unités, sur la même échelle et sont donc intercomparables.\n\n# Implémentation du kmeans\n\nMaintenant nous allons implémenter la méthode du kmeans qui exige un nombre de cluster (groupe) .\nBien entendu cela suppose que l'on a déja une idée du nombre de cluster que l'on souhaite avoir.\nNous verrons par la suite qu'il existe des techniques basées sur la variance intra et inter-groupes qui nous permettrons d'avoir une idée du nombre optimal de cluster suivant la structure de nos données.\nCe nombre tendra a minimisé la variance intra-groupe et à maximiser la variance inter-groupe.\nVoyons donc de près comment tout ça marche.\n\nLa fonction `kmeans()` est relativement simple à utiliser et voici les arguments à fournir au minimum.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans(x, centers = 3, iter.max = 20, nstart = 10)\n```\n:::\n\n\n::: callout-note\n-   x : matrice numérique, dataframe ou tibble numérique ou simplement un vecteur numérique.\n\n-   centers : le nombre de clusters (k), alors un ensemble aléatoire de lignes (distinctes) dans x est choisi comme centres initiaux.\n\n-   iter.max : nombre maximal d'itérations autorisées.\n    La valeur par défaut est 10.\n\n-   nstart : le nombre de partitions de départ aléatoires.\n    Choisir nstart \\> 1 est recommandé.\n:::\n\nLes autres arguments supposent des analyses poussées qui ne sont pas abordées dans cet tutoriel introductif.\nvous aurez à la fin une liste de documents à consulter pour approfondir le sujet.\n\n## Nombre de clusters prédéfinis\n\nComme il faut bien fournir un nombre de cluster, partons sur la base que pour des besoins pratiques nous voulons regroupé nos penguins en trois groupes plutôt homogène sur la base des quatre variables.\nLa sélection des centres de classes se faisant de façon aléatoires, pour garantir la reproductibilité des analyses, nous devons fixé les résultats qui seront fournis par le RNG (random number generator) à travers la fonction `set.seed()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed = 1234)\nkm_res <- kmeans(penguins_num, centers = 3, iter.max = 20, nstart = 10)\n```\n:::\n\n\nVoilà !\nEh oui vous avez ainsi réaliser un kmeans avec R.\nPour avoir une idée des resultats :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(km_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 9\n $ cluster     : int [1:342] 2 2 2 2 2 2 2 2 1 2 ...\n $ centers     : num [1:3, 1:4] 0.66 -1.047 0.656 0.816 0.486 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"1\" \"2\" \"3\"\n  .. ..$ : chr [1:4] \"bill_length_mm\" \"bill_depth_mm\" \"flipper_length_mm\" \"body_mass_g\"\n $ totss       : num 1364\n $ withinss    : num [1:3] 113 122 143\n $ tot.withinss: num 378\n $ betweenss   : num 986\n $ size        : int [1:3] 87 132 123\n $ iter        : int 3\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n```\n:::\n:::\n\n\n-   `km_res$cluster` donne le numéro de cluster de chaque ligne (1,2 ou 3 selon)\n\n-   `km_res$centers` donne les coordonnées du centre de chaque cluster.\n\n-   `km_res$size` donne le nombre d'individu par groupe\n\n-   `km_res$totss` donne la variance totale du jeu de données\n\n-   `km_res$tot.withinss` donne la variance totale intra-groupe\n\n-   `km_res$betweenss` donne la variance totale inter-groupes\n\nNous utiliserons le package `broom`, pour extraire ces informations utiles de l'objet `km_res`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n# extraire les informations de chaque cluster\ntidy(km_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 7\n  bill_length_mm bill_depth_mm flipper_length_mm body_ma…¹  size withi…² cluster\n           <dbl>         <dbl>             <dbl>     <dbl> <int>   <dbl> <fct>  \n1          0.660         0.816            -0.286    -0.374    87    113. 1      \n2         -1.05          0.486            -0.890    -0.769   132    122. 2      \n3          0.656        -1.10              1.16      1.09    123    143. 3      \n# … with abbreviated variable names ¹​body_mass_g, ²​withinss\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# les informations sur les variances\nglance(km_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  <dbl>        <dbl>     <dbl> <int>\n1  1364         378.      986.     3\n```\n:::\n:::\n\n\nNous avons choisi 20 itérations pour le choix des centres.\nLe meilleur partitionnement est obtenu au numéro iter.\n\nAussi il est particulièrement utile de joindre au jeu de données `penguins_num` une variable `.cluster` qui va identifier à quel cluster appartient une observation donnée.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_clustered <- penguins %>% \n  select(-year) %>% \n  select_if(is.numeric) %>% \n  drop_na() %>% \n  augment(km_res, .) %>% \n  left_join(penguins,.) %>% \n  drop_na()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nselect: dropped one variable (year)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nselect_if: dropped 3 variables (species, island, sex)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndrop_na: removed 2 rows (1%), 342 rows remaining\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\",\n\"body_mass_g\")\nleft_join: added one column (.cluster)\n> rows only in x 2\n> rows only in y ( 0)\n> matched rows 342\n> =====\n> rows total 344\ndrop_na: removed 11 rows (3%), 333 rows remaining\n```\n:::\n\n```{.r .cell-code}\nglimpse(penguins_clustered)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 333\nColumns: 9\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm <int> 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       <int> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               <fct> male, female, female, female, male, female, male, fe…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ .cluster          <fct> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2…\n```\n:::\n:::\n\n\n## Estimation du nombre optimal de cluster\n\nComme annoncé plus tôt, comment s'assurer du bon choix de k pour garantir le meilleur clustering possible de notre jeu de données ?\n\nPlusieurs approches existent, mais la plus simple consiste à réaliser le kmeans avec des valeurs différentes de k afin de voir comment évolue la variance intra-groupe.\nEn abcisse on aura le nombre de clusters et en ordonnées la variance intra-groupe.\nLe nombre optimale de cluster se situe la ou se trouve le coude.\nVous l'aurez compris c'est une méthode visuelle : la méthode du coude (elbow).\nLa fonction `fviz_nbclust()` du package `factoextra` permet celà.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed = 1234)\nfviz_nbclust(x = penguins_num,FUNcluster = kmeans,method = \"wss\",k.max = 15)\n```\n\n::: {.cell-output-display}\n![](kmeans_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nOn dira que celà est subjectif, mais le nombre optimal de groupe dans ce cas semble être `k = 3` .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed = 1234)\nfviz_nbclust(x = penguins_num,FUNcluster = kmeans,method = \"wss\",k.max = 15)+\n  geom_vline(xintercept = 3, linetype = \"dashed\",\n             color =\"red\")\n```\n\n::: {.cell-output-display}\n![](kmeans_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nD'autres méthodes comme \"silhouette\" ou \"gap_statistic sont implémentés pour estimer le nombre optimal de cluster k. Pour en savoir plus sur ces méthodes et bien d'autres consulter cet excellent [article](https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92).\n\nLa méthode \"silhouette\" nous suggère deux clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_nbclust(x = penguins_num,FUNcluster = kmeans,method = \"silhouette\",k.max = 15)\n```\n\n::: {.cell-output-display}\n![](kmeans_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nLa méthode \"gap_stat\" nous suggère plutôt quatre groupes !\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_nbclust(x = penguins_num,FUNcluster = kmeans,method = \"gap_stat\",k.max = 15)\n```\n\n::: {.cell-output-display}\n![](kmeans_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nDe toute les façons ce sont des indications pour le clustering.\nOn pourra par aggression visuelle s'assurer de la bonne partition des données.\nNous allons donc entamer un nouveau point à savoir la visualisation des clusters.\n\n# Visualisation des clusters\n\nJusque là, nous avons manipuler le résultat de notre clustering de façon numérique.\nIl serait bon de visualiser le résultat.\nA cet effet, la fonction fviz_cluster() du package factoextra nous permet de visualiser les données avec le moins de code possible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed = 1234)\nkm_res <- kmeans(penguins_num, centers = 3, iter.max = 20, nstart = 10)\n# visualisation\nfviz_cluster(object = km_res,data = penguins_num)+\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](kmeans_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nAvec `k = 3` clusters, visiblement les données ne sont pas bien regroupées.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed = 1234)\nkm_res <- kmeans(penguins_num, centers = 4, iter.max = 20, nstart = 10)\n# visualisation\nfviz_cluster(object = km_res,data = penguins_num)+\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](kmeans_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nPar agression visuelle aussi, les données semblent ne pas être bien partitionnées.\nLes graphiques laissent voir en réalités deux clusters comme nous le proposait la méthode \"silhouette\".\nVous l'aurez compris, il ne s'agit pas juste d'exécuter du code et de prendre les résultats pour de l'argent comptant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed = 1234)\nkm_res <- kmeans(penguins_num, centers = 2, iter.max = 20, nstart = 10)\n# visualisation\nfviz_cluster(object = km_res,data = penguins_num)+\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](kmeans_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nVoilà qui est assez interréssant.\nLes données se regroupent en mieux en deux clusters.\n\n# Conclusion\n\nNous voilà à la fin de ce court tutoriel pour vous introduire au clustering avec R.\nLes notions couvertent ici sont loin de faire le tour de l'état d'art sur la question.\nPour approfondir le sujet je vous recommande les contenus ci après :\n\n[The complete guide to clustering analysis: k-means and hierarchical clustering by hand and in R](https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/) de Antoine Soetewey , c'est de loin à mon avis la ressource la plus complète sur le sujet.\n\n[Articles - Cluster Analysis in R: Practical Guide](http://www.sthda.com/english/articles/25-clusteranalysis-in-r-practical-guide/) de Abdoukalel Kassambara\n\n[K-means Cluster Analysis](https://uc-r.github.io/kmeans_clustering) de Bradley Boehmke\n\n# Reférences\n\n\n  - tidylog (version 1.0.2; Elbers B, 2020)\n  - lubridate (version 1.8.0; Grolemund G, Wickham H, 2011)\n  - purrr (version 0.3.5; Henry L, Wickham H, 2022)\n  - palmerpenguins (version 0.1.1; Horst AM et al., 2020)\n  - factoextra (version 1.0.7; Kassambara A, Mundt F, 2020)\n  - tibble (version 3.1.8; Müller K, Wickham H, 2022)\n  - R (version 4.2.0; R Core Team, 2022)\n  - broom (version 1.0.1; Robinson D et al., 2022)\n  - ggplot2 (version 3.3.6; Wickham H, 2016)\n  - forcats (version 0.5.2; Wickham H, 2022)\n  - stringr (version 1.4.1; Wickham H, 2022)\n  - tidyverse (version 1.3.2.9000; Wickham H et al., 2019)\n  - dplyr (version 1.0.10; Wickham H et al., 2022)\n  - tidyr (version 1.2.1; Wickham H, Girlich M, 2022)\n  - readr (version 2.1.3; Wickham H et al., 2022)\n",
    "supporting": [
      "kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}